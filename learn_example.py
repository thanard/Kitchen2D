# Author: Zi Wang
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

try:
   import cPickle as pickle
except:
   import pickle
import os
import active_learners.helper as helper
import h5py
import numpy as np
from active_learners.active_learner import run_ActiveLearner


def gen_data(expid, exp, n_data, save_fnm):
    '''
    Generate initial data for a function associated the experiment.
    Args:
        expid: ID of the experiment; e.g. 0, 1, 2, ...
        exp: name of the experiment; e.g. 'pour', 'scoop'.
        n_data: number of data points to generate.
        save_fnm: a file name string where the initial data will be
        saved.
    '''
    print('Generating data...')
    func = helper.get_func_from_exp(exp)
    xx, yy = helper.gen_data(func, n_data)
    pickle.dump((xx, yy), open(save_fnm, 'wb'))

def run_exp(expid, exp, method, n_init_data, iters):
    '''
    Run the active learning experiment.
    Args:
        expid: ID of the experiment; e.g. 0, 1, 2, ...
        exp: name of the experiment; e.g. 'pour', 'scoop'.
        method: learning method, including 
            'nn_classification': a classification neural network 
                based learning algorithm that queries the input that has 
                the largest output.
            'nn_regression': a regression neural network based 
                learning algorithm that queries the input that has 
                the largest output.
            'gp_best_prob': a Gaussian process based learning algorithm
                that queries the input that has the highest probability of 
                having a positive function value.
            'gp_lse': a Gaussian process based learning algorithm called
                straddle algorithm. See B. Bryan, R. C. Nichol, C. R. Genovese, 
                J. Schneider, C. J. Miller, and L. Wasserman, "Active learning for 
                identifying function threshold boundaries," in NIPS, 2006.
            'random': an algorithm that query uniformly random samples.
        n_data: number of data points to generate.
        save_fnm: a file name string where the initial data will be
        saved.
    '''
    dirnm = 'data/'
    if not os.path.isdir(dirnm):
        os.mkdir(dirnm)
    init_fnm = os.path.join(
            dirnm, '{}_init_data_{}.pk'.format(exp, expid))
    gen_data(expid, exp, n_init_data, init_fnm)

    initx, inity = pickle.load(open(init_fnm, 'rb'))

    func = helper.get_func_from_exp(exp)

    active_learner = helper.get_learner_from_method(method, initx, inity, func)

    # file name for saving the learning results
    learn_fnm = os.path.join(
            dirnm, '{}_{}_{}.pk'.format(exp, method, expid))

    # get a context
    context = helper.gen_context(func)

    # start running the learner
    print('Start running the learning experiment...')
    run_ActiveLearner(active_learner, context, learn_fnm, iters)

def sample_exp(expid, exp, method, n_eps, n_timesteps_per_ep):
    '''
    Sample from the learned model.
    Args:
        expid: ID of the experiment; e.g. 0, 1, 2, ...
        exp: name of the experiment; e.g. 'pour', 'scoop'.
        method: see run_exp.
    '''
    func = helper.get_func_from_exp(exp)
    xx, yy, c = helper.get_xx_yy(expid, method, exp=exp)
    active_learner = helper.get_learner_from_method(method, xx, yy, func)
    active_learner.retrain()
    # Enable gui
    func.do_gui = True
    # while raw_input('Continue? [y/n]') == 'y':
    all_ims = []
    all_actions = []
    for i in range(n_eps):
        print("##### %s: %d ######" % (method, i))
        x = active_learner.sample(c)
        import ipdb
        ipdb.set_trace()
        ims, actions = func(x, n_timesteps_per_ep)
        # func(x)
        all_ims.append(ims)
        all_actions.append(actions)
    return np.array(all_ims), np.array(all_actions)

if __name__ == '__main__':
    exp = 'pour'
    methods = ['gp_lse']#, 'random']
    expid = 0
    n_eps = 500
    n_timesteps_per_ep = 40
    # n_init_data = 10
    # iters = 50
    save_data = True
    dataset = h5py.File('/usr/local/google/home/thanard/data/pouring.hdf5', 'w')
    sim_data = dataset.create_group('sim')
    sim_data.create_dataset("ims", (n_eps, n_timesteps_per_ep, 64, 64, 3), dtype='f')
    sim_data.create_dataset("actions", (n_eps, n_timesteps_per_ep, 3), dtype='f')

    # run_exp(expid, exp, method, n_init_data, iters)
    i = 0
    for method in methods:
        ims, actions = sample_exp(expid, exp, method, n_eps//len(methods), n_timesteps_per_ep)
        # import ipdb
        # ipdb.set_trace()
        dataset['sim']['ims'][i:i+n_eps//len(methods)] = ims
        dataset['sim']['actions'][i:i+n_eps//len(methods)] = actions
        i += n_eps//len(methods)
